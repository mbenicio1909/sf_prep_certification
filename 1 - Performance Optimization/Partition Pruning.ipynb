{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "id": "98eb2244-08d4-4bae-a74e-e98928d4e65b",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "# Micro partitions\n\nSnowflake stores a table in many small chunks called micro-partitions. \nFor each micro-partition, Snowflake keeps metadata like the MIN and MAX values of each column inside that chunk.\n \n * 50-500 MB of uncompressed data\n * Data stored in columnar format, not rows.\n * Repetition of column range of data can happen, causing ***overlapping***\n  ",
      "execution_count": null
    },
    {
      "id": "411019da-4620-48bd-9a75-aff12c46fc16",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "## Overlapping micro-partitions\n\nIf we take on one column (example: order_date), each micro-partition has a range:\n\nMicro-partition P1 contain dates from Jan 1 to Jan 10\nMicro-partition P2 contain dates from Jan 11 to Jan 20\n\nThat is 0 data overlapping.\n\nHowever, \nP1: [Jan 01 … Jan 20]\nP2: [Jan 10 … Feb 05]\nP3: [Jan 15 … Jan 25]\n\nJan 15 apperrs in the range of P1, P2 and P3 causing range overlap.\nSo, in case you search for order_date = jan 15. all 3 partitions will be scanned.\n\n\n## Logical vs Physical Structure\n\n![image info](https://docs.snowflake.com/en/_images/tables-clustered1.png)\n"
    },
    {
      "id": "a5be82b2-7e6f-475d-b13f-293e4c05ba07",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_2",
        "name": "creating sample tables",
        "title": "creating sample tables",
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "%%sql -r dataframe_2\n\nuse role sysadmin;\ncreate or replace database sf_cert_prep;\n\n--creating a table, forcing to be order by o_comment, even knowing all searches will be on o_orderdate.\ncreate or replace table sf_cert_prep.public.t_orders_bad\nas \nselect * from SNOWFLAKE_SAMPLE_DATA.TPCH_SF10.orders order by o_comment  asc ;\n\n\n--creating a table, forcing to be order by o_orderdate\ncreate or replace table sf_cert_prep.public.t_orders_good\nas \nselect * from SNOWFLAKE_SAMPLE_DATA.TPCH_SF10.orders order by o_orderdate  asc ;\n\n--creating a table, forcing to be order by o_comment, even knowing all searches will be on o_orderdate, but adding cluster key as orderdate\ncreate or replace table sf_cert_prep.public.t_orders_bad_but_clustered \ncluster by (o_orderdate)\nas \nselect * from SNOWFLAKE_SAMPLE_DATA.TPCH_SF10.orders order by o_comment  asc ;\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "be064fc0-d157-439d-90a6-3a8aae4864d1",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_6",
        "language": "sql",
        "collapsed": false,
        "codeCollapsed": false,
        "name": "t_orders_bad - Analyze % partitions scanned",
        "title": "t_orders_bad - Analyze % partitions scanned"
      },
      "source": "%%sql -r dataframe_6\n\nALTER SESSION SET USE_CACHED_RESULT = FALSE; -- disabling query cache\n-- cleaning warehouse cache\nALTER WAREHOUSE COMPUTE_WH SUSPEND;\nALTER WAREHOUSE COMPUTE_WH RESUME;\n\nselect * from sf_cert_prep.public.t_orders_bad where o_orderdate = '1993-09-08';\nselect --OPERATOR_STATISTICS,\nOPERATOR_STATISTICS:pruning.partitions_scanned::integer as partitions_scanned,\nOPERATOR_STATISTICS:pruning.partitions_total::integer  as partitions_total , \npartitions_scanned/partitions_total as overlap from table(get_query_operator_stats(last_query_id(-1))) where operator_type = 'TableScan';",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c8288e1f-0e0c-489d-a98e-be3aedf46040",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_11",
        "language": "sql",
        "collapsed": false,
        "codeCollapsed": false,
        "name": "t_orders_good - Analyze % partitions scanned",
        "title": "t_orders_good - Analyze % partitions scanned"
      },
      "source": "%%sql -r dataframe_11\n\n\nALTER SESSION SET USE_CACHED_RESULT = FALSE; -- disabling query cache\n-- cleaning warehouse cache\nALTER WAREHOUSE COMPUTE_WH SUSPEND;\nALTER WAREHOUSE COMPUTE_WH RESUME;\n\nselect * from sf_cert_prep.public.t_orders_good where o_orderdate = '1993-09-08';\nselect --OPERATOR_STATISTICS,\nOPERATOR_STATISTICS:pruning.partitions_scanned::integer as partitions_scanned,\nOPERATOR_STATISTICS:pruning.partitions_total::integer  as partitions_total , \npartitions_scanned/partitions_total as overlap from table(get_query_operator_stats(last_query_id(-1))) where operator_type = 'TableScan';",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "48c26de7-0b62-45f4-a361-1b71b31e466a",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_14",
        "language": "sql",
        "collapsed": false,
        "codeCollapsed": false,
        "name": "t_orders_bad_but_clustered - Analyze % partitions scanned",
        "title": "t_orders_bad_but_clustered - Analyze % partitions scanned"
      },
      "source": "%%sql -r dataframe_14\n\n\nALTER SESSION SET USE_CACHED_RESULT = FALSE; -- disabling query cache\n-- cleaning warehouse cache\nALTER WAREHOUSE COMPUTE_WH SUSPEND;\nALTER WAREHOUSE COMPUTE_WH RESUME;\n\nselect * from sf_cert_prep.public.t_orders_bad_but_clustered where o_orderdate = '1993-09-08';\nselect --OPERATOR_STATISTICS,\nOPERATOR_STATISTICS:pruning.partitions_scanned::integer as partitions_scanned,\nOPERATOR_STATISTICS:pruning.partitions_total::integer  as partitions_total , \npartitions_scanned/partitions_total as overlap from table(get_query_operator_stats(last_query_id(-1))) where operator_type = 'TableScan';",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b835cab7-4c11-4c2e-afb4-43e25a5f28df",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "### Explanation\n\nUsually we dont have the luxury to rewrite a production table the way we inteded. Merge,updates or reloads of the source, will mix up and mess up the way partition were \"optimized\".\nAs alternative for those cases, we can use \"clustering keys\". Clustering keys will create new partitions based on the keys or expressions that we provide.\n* Old paritions will remain unchanged, if not used anymore, they will be marked to be deleted after timetravel / fail safe period\n* New partitons will have priority in the metadata / query plan\n* Re-clustering will not recreate all paritions, only the ones that will benefit the most\n",
      "execution_count": null
    },
    {
      "id": "f68b7db8-c0cb-4f18-8c34-899480c27001",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## clustering_information function break down.\n\nReturns clustering information, including average clustering depth, for a table based on one or more columns in the table.\n\n### Arguments:\n\n1 - table_name - Table for which you want to return clustering information. \n\n2 - expression or columns:\n\nColumn names or expressions for which clustering information is returned:\n\n- no clustering key -> this argument is required. If this argument is omitted, an error is returned.\n\n- with clustering key -> optional; \n\n\n***Even if only one column name or expression is passed, it must be inside parentheses.***\n\n3 - number_of_errors\n\nNumber of clustering errors returned by the function. If this argument is omitted, the 10 most recent errors are returned\n\n\n#### List of Returns - not all listed, only most important ones. -> check documentation for full list: https://docs.snowflake.com/en/sql-reference/functions/system_clustering_information\n\n* cluster_by_keys -> Columns in table used to return clustering information\n* total_partition_count -> Total number of micro-partitions that comprise the table.\n* total_constant_partition_count -> Total number of micro-partitions for which the value of the specified columns have reached a constant state. The most constant MP is, more efficient is the prunning.\n* average_overlaps -> Average number of overlapping micro-partitions for each micro-partition in the table. A high number indicates the table is not well-clustered.\n* average_depth -> Average overlap depth of each micro-partition in the table. A high number indicates the table is not well-clustered.\n* partition_depth_histogram -> A histogram depicting the distribution of overlap depth for each micro-partition in the table\n\n\n\n***Important***\nIF the table has more than 2 million partitions this analysis is based in a subset.\n\n\n\n\n",
      "execution_count": null
    },
    {
      "id": "781f48db-14dc-45ce-913f-ff3786a6966a",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_12",
        "language": "sql",
        "collapsed": false,
        "codeCollapsed": false,
        "name": "base usage",
        "title": "base usage"
      },
      "source": "%%sql -r dataframe_12\nselect system$clustering_information('sf_cert_prep.public.t_orders_bad', '(o_orderdate)') as clust_info",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e782cd69-d374-49f2-bf1a-5ed184abfb26",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_16",
        "language": "sql",
        "name": "check on information schema",
        "title": "check on information schema"
      },
      "source": "%%sql -r dataframe_16\nselect TABLE_NAME, CLUSTERING_KEY, AUTO_CLUSTERING_ON from sf_cert_prep.information_schema.tables where table_schema = 'PUBLIC';",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "696b9686-0818-4348-936b-3591ad774ff6",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_8",
        "language": "sql",
        "name": "comparison on average_dept / average_overlaps",
        "title": "comparison on average_dept / average_overlaps",
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "%%sql -r dataframe_8\nwith clust_info as \n(\nselect 'T_ORDERS_BAD' AS TABLE_NAME, parse_json(system$clustering_information('sf_cert_prep.public.T_ORDERS_BAD', '(o_orderdate)'))::variant as c_info\nUNION ALL\nselect 'T_ORDERS_GOOD' AS TABLE_NAME,parse_json(system$clustering_information('sf_cert_prep.public.T_ORDERS_GOOD', '(o_orderdate)'))::variant as c_info\nUNION ALL\nselect 'T_ORDERS_BAD_BUT_CLUSTERED' AS TABLE_NAME,parse_json(system$clustering_information('sf_cert_prep.public.T_ORDERS_BAD_BUT_CLUSTERED', '(o_orderdate)'))::variant as c_info\n)\n\nselect \nTABLE_NAME,\nc_info:cluster_by_keys as cluster_by_keys,\nc_info:total_partition_count as total_partition_count,\nc_info:total_constant_partition_count as total_constant_partition_count,\nc_info:average_depth as average_depth,\nc_info:average_overlaps as average_overlaps,\nc_info:notes as notes,\nc_info  from clust_info\n;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dcc8123f-7aef-412c-a2c3-2b7de807dfa5",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "Analysing the histogram",
        "title": "Analysing the histogram"
      },
      "source": "import pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "80cf3356-bcbe-4ae1-a631-aa5548c8a350",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "\ntable_name = 'sf_cert_prep.public.t_orders_bad'\n#table_name = 'sf_cert_prep.public.t_orders_good'\n#table_name = 'sf_cert_prep.public.t_orders_bad_but_clustered'\n\ncolumn_name = '(o_orderdate)'\n\nquery = f'''\nwith clust_info as \n(select parse_json(system$clustering_information(\\'{table_name}\\', \\'{column_name}\\'))::variant as c_info),\n\nhist as\n(select c_info:partition_depth_histogram as partition_depth_histogram from clust_info )\n\nSELECT  \n  TRY_TO_NUMBER(f.key::string)                 AS partition_depth,       -- transform to in int for the histogram chart\n  f.value::number                              AS count\nFROM hist,\nLATERAL FLATTEN(input => partition_depth_histogram) AS f\nORDER BY partition_depth NULLS LAST;\n'''\n\n# print(query)\n\ndf_hist = session.sql(query).to_pandas()  # Snowpark -> Pandas\n\n\nfig = px.bar(\n    df_hist,\n    x=\"PARTITION_DEPTH\",\n    y=\"COUNT\",\n    text=\"COUNT\",\n    title=\"Partition Depth Histogram (Aggregated)\",\n    labels={\"PARTITION_DEPTH\": \"Partition Depth\", \"COUNT\": \"Count\"},\n)\nfig.update_traces(textposition=\"outside\", cliponaxis=False)\nfig.update_layout(xaxis=dict(dtick=1), yaxis_title=\"Count\", bargap=0.15)\nfig.show()\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "354060b0-5aae-4297-9f91-eee9b9a20055",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "### Explanation\n\n***Important!!***\n\nThere are 24 partitions in total.\n\nFrom the 16th bucket, Snowflake starts incrmenting the depth number as twice the width of the previous bucket.\nSo in the t_orders_bad case, it will shown that all the 24 micro-partitions as depth 32, but in reality it should be 23 depth.\n\n\n### Number of Overlap vs Overlap depth\n\n![image info](https://docs.snowflake.com/en/_images/tables-clustering-ratio.png)\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7814d2af-2777-4a5f-a1ca-f929c3a42e54",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "source": "\ntable_name = 'sf_cert_prep.public.t_orders_bad_but_clustered'\ncolumn_name = '(o_orderdate)'\n\nquery = f'''\nwith clust_info as \n(select parse_json(system$clustering_information(\\'{table_name}\\', \\'{column_name}\\'))::variant as c_info),\n\nhist as\n(select c_info:partition_depth_histogram as partition_depth_histogram from clust_info )\n\nSELECT  \n  TRY_TO_NUMBER(f.key::string)                 AS partition_depth,       -- transform to in int for the histogram chart\n  f.value::number                              AS count\nFROM hist,\nLATERAL FLATTEN(input => partition_depth_histogram) AS f\nORDER BY partition_depth NULLS LAST;\n'''\n\n# print(query)\n\ndf_hist = session.sql(query).to_pandas()  # Snowpark -> Pandas\n\n\nfig = px.bar(\n    df_hist,\n    x=\"PARTITION_DEPTH\",\n    y=\"COUNT\",\n    text=\"COUNT\",\n    title=\"Partition Depth Histogram (Aggregated)\",\n    labels={\"PARTITION_DEPTH\": \"Partition Depth\", \"COUNT\": \"Count\"},\n)\nfig.update_traces(textposition=\"outside\", cliponaxis=False)\nfig.update_layout(xaxis=dict(dtick=1), yaxis_title=\"Count\", bargap=0.15)\nfig.show()\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dcf2608d-f08d-44fc-a19f-bd7d2d5ea699",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": false,
        "collapsed": false
      },
      "source": "## Automatic Clustering\n\n### Basics:\n* it will not start start automatically, Snowflake only reclusters a clustered table if it will benefit from the operation.\n* Snowflake performs automatic reclustering in the background, and you do not need to specify a warehouse to use. (serverless compute resources))\n* Non-blocking DML\n\n### Costs:\n\nCompute -> The more the table is changed, higher the costs.\nStorage -> reoganization will increase storage costs, but once the timetravel/failsafe period expires, old partitions will be dropped, causing \"almost a net 0\"."
    },
    {
      "id": "fbc86378-c40c-4fcd-ae9a-a43813789b26",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_1",
        "language": "sql",
        "collapsed": false,
        "codeCollapsed": false,
        "name": "Suspend",
        "title": "Suspend"
      },
      "source": "%%sql -r dataframe_1\n\n\nuse sf_cert_prep.public;\nALTER TABLE t_orders_bad_but_clustered SUSPEND RECLUSTER;\nSHOW TABLES LIKE 't_orders_bad_but_clustered';\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bed7723f-daaa-405d-84c3-2d0d5882c066",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_19",
        "language": "sql",
        "name": "Enabling",
        "title": "Enabling"
      },
      "source": "%%sql -r dataframe_19\nuse sf_cert_prep.public;\nALTER TABLE t_orders_bad_but_clustered RESUME  RECLUSTER;\nSHOW TABLES LIKE 't_orders_bad_but_clustered';",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9f78523d-b1e9-4f14-aa2a-ca5092a48552",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_20",
        "language": "sql",
        "name": "checking costs",
        "title": "checking costs"
      },
      "source": "%%sql -r dataframe_20\n-- data comes from snowflake.account_usage, so expect latency of 45m to 3h\n\n\nSELECT TO_DATE(start_time) AS date,\n  database_name,\n  schema_name,\n  table_name,\n  SUM(credits_used) AS credits_used\nFROM snowflake.account_usage.automatic_clustering_history\nWHERE start_time >= DATEADD(month,-1,CURRENT_TIMESTAMP())\nGROUP BY 1,2,3,4\nORDER BY 5 DESC;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1bfda6b5-ff6a-455b-a9bc-9e981194a41a",
      "cell_type": "markdown",
      "metadata": {},
      "source": "### ESTIMATE_AUTOMATIC_CLUSTERING_COSTS\n\n- it will return 2 costs, the intial value and the maintenance.\n- If table created recently the maintenance estimation will return a comment saying that is best have at least 7 days to have a proper estimation\n- uses samples from a subset of micro-paritions, so it is not guaranteed that will be the accurate\n- for extra accurancy, needs to run multiple times and use the average of the results\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9817d2e4-4888-4e12-a4f2-a7f6ab68e4d7",
      "cell_type": "code",
      "metadata": {
        "language": "python",
        "name": "estimating costs",
        "title": "estimating costs"
      },
      "source": "\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\ntable_name = 'sf_cert_prep.public.t_orders_bad'\ncolumn_name = '(o_orderdate)'\n\nquery = f'''\nSELECT SYSTEM$ESTIMATE_AUTOMATIC_CLUSTERING_COSTS(\\'{table_name}\\', \\'{column_name}\\'); '''\n\n#print(query)\nsession.sql(query)\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1157e806-4db0-40fe-8705-4775d659a821",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_22",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_22\nSELECT SYSTEM$ESTIMATE_AUTOMATIC_CLUSTERING_COSTS('sf_cert_prep.public.t_orders_bad_but_clustered', '(o_orderdate)');",
      "outputs": [],
      "execution_count": null
    }
  ]
}