{
  "metadata": {
    "kernelspec": {
      "display_name": "Jupyter Notebook",
      "name": "jupyter"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "9e3a8fee-cb23-4160-ac44-2386aed54372",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "## Copy\n\nLoads data into a table or into a location.\n\nCOPY INTO <table>\n\nLoads data files to an existing table.\n\nFrom:\n\n- Named Internal Stage\n- Named External Stage\n- Table Stage \n- User Stage\n- External Location: bypass of External stage.\n    - AWS:  COPY INTO mytable FROM 's3://mybucket/./../a.csv';\n    - GCP:  COPY INTO mytable FROM 'gcs://mybucket/./../a.csv'; \n    - Azure COPY INTO mytable FROM 'azure://myaccount.blob.core.windows.net/mycontainer/./../a.csv';\n\n    \n\nBase Syntax:\n\n    COPY INTO [<namespace>.]<table_name>\n         FROM { internalStage | externalStage | externalLocation }\n    [ FILES = ( '<file_name>' [ , '<file_name>' ] [ , ... ] ) ]\n    [ PATTERN = '<regex_pattern>' ]\n    [ FILE_FORMAT = ( { FORMAT_NAME = '[<namespace>.]<file_format_name>' |\n                        TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML } [ formatTypeOptions ] } ) ]\n    [ copyOptions ]\n    [ VALIDATION_MODE = RETURN_<n>_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS ]\n\nApplying basic data Transformation:\n\n\n    COPY INTO [<namespace>.]<table_name> [ ( <col_name> [ , <col_name> ... ] ) ]\n         FROM ( SELECT [<alias>.]$<file_col_num>[.<element>] [ , [<alias>.]$<file_col_num>[.<element>] ... ]\n                FROM { internalStage | externalStage } )\n    [ FILES = ( '<file_name>' [ , '<file_name>' ] [ , ... ] ) ]\n    [ PATTERN = '<regex_pattern>' ]\n    [ FILE_FORMAT = ( { FORMAT_NAME = '[<namespace>.]<file_format_name>' |\n                        TYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML } [ formatTypeOptions ] } ) ]\n  \n\n  \n\n\nOptional Parameters:\n1. Files\n\n    [ FILES = ( '<file_name>' [ , '<file_name>' ] [ , ... ] ) ]\n\nList of file names separeted by comma. \nFor internal stages: Name of the file only.\nFor external stages: the file path is set by concatenating the URL in the stage definition and the list of resolved file names.\n\nLimit of 1000 files that can be specified.\n\n\n2. Pattern\n    PATTERN = 'regex_pattern'\n\n\nA regular expression pattern string.\n\n\n3. File Format \n    You can manually specify the format, or use a named format name.\n\n    FILE_FORMAT = ( FORMAT_NAME = 'file_format_name' )\n\n    OR \n    \n    FILE_FORMAT = ( TYPE = CSV | JSON | AVRO | ORC | PARQUET | XML [ ... ] )\n\n\n4. Validation Mode:\n\n     RETURN_n_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS\n\n Instructs the COPY command to validate the data files instead of loading them into the specified table.\n\n    - RETURN_n_ROWS = Return N number of rows\n    - RETURN_ERRORS = return all errors of this specific copy command.\n    - RETURN_ALL_ERRORS = RETURN_ERRORS + files with errors that were partially loaded during an earlier load because the ON_ERROR copy option was set to CONTINUE during the load.\n\n***Important***\n- VALIDATION_MODE does not support COPY statements that transform data during a load.\n- VALIDATION_MODE isn’t supported for Iceberg tables.\n- Use the VALIDATE table function to view all errors encountered during a previous load. Note that this function also does not support COPY statements that transform data during a load.\n\n",
      "execution_count": null
    },
    {
      "id": "fdf4b403-2122-4e88-9a0b-75590b156b06",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": true
      },
      "source": "\n### Copy Options\n\n### CLUSTER_AT_INGEST_TIME = TRUE | FALSE\n\n    Default = False\n\nPre-cluster data directly during ingestion for tables that are configured with clustering keys -> important for partition pruning.   \n\n        \n### ENFORCE_LENGTH / TRUNCATECOLUMNS  = TRUE | FALSE\n\n    ENFORCE_LENGTH   = Default = True\n    TRUNCATECOLUMNS  = Default = False\n\nDecides if copy will break if string in the file is bigger than the table definition \n            \n         \n### FILE_PROCESSOR = (SCANNER = <custom_scanner_type> SCANNER_OPTIONS = (<scanner_options>))\n\nSpecifies the scanner and the scanner options that are used for processing unstructured data\n\n### FORCE = TRUE | FALSE\n\n    DEFAULT = FALSE\n\nLoads all file, even if the files were already loaded before (14 days period)\n         \n### INCLUDE_METADATA = ( <column_name> = METADATA$<field> [ , <column_name> = METADATA${field} ... ] )\n    \n    DEFAULT = NULL\n    \nA user-defined mapping between a target table’s existing columns to its METADATA$ columns\n- METADATA$FILENAME\n- METADATA$FILE_ROW_NUMBER\n- METADATA$FILE_CONTENT_KEY\n- METADATA$FILE_LAST_MODIFIED\n- METADATA$START_SCAN_TIME\n\n\nImportant:\n- The INCLUDE_METADATA target column name must first exist in the table. The target column name is not automatically added if it doesn’t exist.\n- Use a unique column name for the INCLUDE_METADATA columns.\n- If name conflict with a column in the data file, the METADATA$ value that is defined by INCLUDE_METADATA takes precedence\n- When you load a CSV file with INCLUDE_METADATA, set the file format option ERROR_ON_COLUMN_COUNT_MISMATCH to FALSE.\n            \n         \n###         LOAD_MODE = { FULL_INGEST | ADD_FILES_COPY }\n\n        DEFAULT = FULL_INGEST \n\nSpecifies the mode to use when you load data from Parquet files into a Snowflake-managed Iceberg table.\n         \n###         LOAD_UNCERTAIN_FILES = TRUE | FALSE\n\n         DEFAULT = FALSE \n\nSkips files when load status is unknown.\n\nThe load status is unknown if all of the following conditions are true:\n\n- The file’s LAST_MODIFIED date (that is, the date when the file was staged) is older than 64 days.\n- The initial set of data was loaded into the table more than 64 days earlier.    \n- If the file was already loaded successfully into the table, this event occurred more than 64 days earlier.\n         \n ###        MATCH_BY_COLUMN_NAME = CASE_SENSITIVE | CASE_INSENSITIVE | NONE\n\n        DEFAULT = None\n\n String that specifies whether to load semi-structured data into columns in the target table that match corresponding columns represented in the data.\n Does not work with copy transform.\n\n Works with the data formats:\n\n This copy option is supported for the following data formats:\n\n- JSON        \n- Avro        \n- ORC        \n- Parquet        \n- CSV\n\nConditions:\n- The column must have the exact same name as the column in the table.\n- Column order does not matter\n- The column in the table must have a data type that is compatible with the values in the column represented in the data.\n    For example, string, number, and Boolean values can all be loaded into a variant column.         \n\n            \n###         ON_ERROR = { CONTINUE | SKIP_FILE | SKIP_FILE_<num> | 'SKIP_FILE_<num>%' | ABORT_STATEMENT }\n\n         DEFAULT Copy = ABORT_STATEMENT   \n         DEFAULT Snowpipe = SKIP_FILE   \n\nCONTINUE -> Continue to load the file if errors are found\nSKIP_FILE-> Skip a file when an error is found\n    SKIP_FILE_10 -> Skip 10 files\n    SKIP_FILE_10% -> Skip 10% of the files\nABORT_STATEMENT ->  Stop the load operation if any error is found in a data file   \n            \n         \n###         PURGE = TRUE | FALSE\n\n         DEFAULT = FALSE\n\nRemove the data files from the stage automatically after the data is loaded successfully.\n\nIf the purge operation fails for any reason, no error is returned. \n         \n ###        RETURN_FAILED_ONLY = TRUE | FALSE\n\n         DEFAULT = FALSE\n\nReturn only files that have failed to load in the statement result.\n\n         \n ###        SIZE_LIMIT = <num>\n\n         DEFAULT = NULL\n\nMaximum size in bytes to be loaded.\n       "
    },
    {
      "id": "f4f90924-d1c6-47ce-97e0-f7f4b8dd2f81",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_5",
        "language": "sql",
        "name": "Create database",
        "title": "Create database"
      },
      "source": "%%sql -r dataframe_5\nCREATE DATABASE IF NOT EXISTS SF_CERT_PREP;\nUSE SF_CERT_PREP;",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "c4a7bc0d-1ee1-4fe5-a395-a95af182e9fe",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_1",
        "name": "Create named external stage",
        "title": "Create named external stage"
      },
      "source": "%%sql -r dataframe_1\nCREATE OR REPLACE STAGE SF_CERT_PREP.public.aws_stage\n    url='s3://bucketsnowflakes3'\n      DIRECTORY = (ENABLE = TRUE) -- better to see the file metadata, but not mandatory   \n    ;\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "id": "8e957c50-26ae-43c8-8430-ddaddf256e68",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_2",
        "language": "sql",
        "name": "List stage",
        "title": "List stage"
      },
      "source": "%%sql -r dataframe_2\nLIST @SF_CERT_PREP.public.aws_stage;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "19ad2ce9-bb4a-4a7f-825a-08335752be74",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_17",
        "language": "sql",
        "name": "Loan_payments_data",
        "title": "Loan_payments_data"
      },
      "source": "%%sql -r dataframe_17\nselect $1,$2, $3 from @SF_CERT_PREP.public.aws_stage/Loan_payments_data.csv;\n\n-- By default SF uses CSV as file format.",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "85a3f8ac-ba8a-4184-8f62-08ee05020b98",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_19",
        "language": "sql",
        "name": "OrderDetails",
        "title": "OrderDetails"
      },
      "source": "%%sql -r dataframe_19\nselect $1,$2, $3 from @SF_CERT_PREP.public.aws_stage/OrderDetails.csv;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d0aecf64-b678-46b5-8d81-05566aee825d",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_20",
        "language": "sql",
        "name": "sampledata",
        "title": "sampledata"
      },
      "source": "%%sql -r dataframe_20\nselect $1,$2, $3 from @SF_CERT_PREP.public.aws_stage/sampledata.csv;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5cca2d3c-2bc8-4d44-92f4-911e9f5e4582",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_25",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_25\ncreate or replace file format SF_CERT_PREP.public.ff_csv_skip_header\ntype = csv\nfield_delimiter=','\nskip_header=1;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9581ba6e-5af7-48b5-a607-ac04bdebdd82",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_10",
        "language": "sql",
        "name": "Create named file format for CSV, skipping 1 line of header",
        "title": "Create named file format for CSV, skipping 1 line of header"
      },
      "source": "%%sql -r dataframe_10\ncreate or replace file format SF_CERT_PREP.public.ff_csv_parse_header\ntype = csv\nfield_delimiter=','\nPARSE_HEADER = TRUE -- Mandatory for when ingesting metadata;\nERROR_ON_COLUMN_COUNT_MISMATCH = FALSE   -- mandatory  for when ingesting metadata;!",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "21959dfa-3867-4917-8309-118488dd2622",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_3",
        "language": "sql",
        "name": "Create Table  to store the data",
        "title": "Create Table  to store the data"
      },
      "source": "%%sql -r dataframe_3\nCREATE OR REPLACE TABLE SF_CERT_PREP.public.ORDERS (\n    ORDER_ID VARCHAR(30),\n    AMOUNT INT,\n    PROFIT INT,\n    QUANTITY INT,\n    CATEGORY VARCHAR(30),\n    SUBCATEGORY VARCHAR(30)\n)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b2d65489-ede6-4305-ae4e-852a6f5e2200",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_11",
        "language": "sql",
        "name": "Base Copy into - file list",
        "title": "Base Copy into - file list"
      },
      "source": "%%sql -r dataframe_11\nCOPY INTO SF_CERT_PREP.public.ORDERS\nFROM @SF_CERT_PREP.public.aws_stage\nfile_format =SF_CERT_PREP.public.ff_csv_skip_header\nfiles = ('OrderDetails.csv');",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a8f8c35f-e74f-4eed-85c7-f5219f0d19cb",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_21",
        "language": "sql",
        "name": "Base Copy into - pattenrns",
        "title": "Base Copy into - pattenrns"
      },
      "source": "%%sql -r dataframe_21\nCOPY INTO SF_CERT_PREP.public.ORDERS\nFROM @SF_CERT_PREP.public.aws_stage\nfile_format =SF_CERT_PREP.public.ff_csv_skip_header\npattern ='OrderDetails.csv';\n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a6dc82ec-ffc7-4fdf-87cd-3327f479552f",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_23",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_23\n-- The same file will not be loaded again.\n-- Option 1 truncate table\n-- Option 2 force=true\n\ntruncate table SF_CERT_PREP.public.ORDERS;\n \nCOPY INTO SF_CERT_PREP.public.ORDERS\nFROM @SF_CERT_PREP.public.aws_stage\nfile_format =SF_CERT_PREP.public.ff_csv_skip_header\npattern ='OrderDetails.csv'\nforce=true\n;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4a03d2c0-eea7-4384-b8a1-d4b5e7a520ee",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## Metadata Columns\n\n### METADATA$FILENAME\nName of the staged data file the current row belongs to. Includes the full path to the data file.\n\n### METADATA$FILE_ROW_NUMBER\nRow number for each record in the staged data file.\n\n### METADATA$FILE_CONTENT_KEY\nChecksum of the staged data file the current row belongs to.\n\n#### METADATA$FILE_LAST_MODIFIED\nLast modified timestamp of the staged data file the current row belongs to. Returned as TIMESTAMP_NTZ.\n\n#### METADATA$START_SCAN_TIME\nStart timestamp of operation for each record in the staged data file. Returned as TIMESTAMP_LTZ.",
      "execution_count": null
    },
    {
      "id": "bded866d-59eb-4e1a-adb7-4ae5cefae636",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_22",
        "language": "sql",
        "name": "Add metadata columns",
        "title": "Add metadata columns"
      },
      "source": "%%sql -r dataframe_22\nCREATE OR REPLACE TABLE SF_CERT_PREP.public.ORDERS (\n    ORDER_ID VARCHAR(30),\n    AMOUNT INT,\n    PROFIT INT,\n    QUANTITY INT,\n    CATEGORY VARCHAR(30),\n    SUBCATEGORY VARCHAR(30),\n    META_FILENAME TEXT,\n    META_FILE_ROW_NUMBER INT,\n    META_FILE_CONTENT_KEY TEXT,\n    META_FILE_LAST_MODIFIED TIMESTAMP_LTZ,\n    META_START_SCAN_TIME TIMESTAMP_LTZ    \n)",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ca0b40c4-16e4-4308-bfac-c56f78e2dca2",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_14",
        "language": "sql",
        "name": "include_metadata",
        "title": "include_metadata"
      },
      "source": "%%sql -r dataframe_14\n COPY INTO SF_CERT_PREP.public.ORDERS\n     FROM @SF_CERT_PREP.public.aws_stage\n     file_format =SF_CERT_PREP.public.ff_csv_parse_header\n      pattern ='OrderDetails.csv'\n      INCLUDE_METADATA = (\nMETA_FILENAME=METADATA$FILENAME\nMETA_FILE_ROW_NUMBER=METADATA$FILE_ROW_NUMBER\nMETA_FILE_CONTENT_KEY=METADATA$FILE_CONTENT_KEY\nMETA_FILE_LAST_MODIFIED= METADATA$FILE_LAST_MODIFIED\nMETA_START_SCAN_TIME=METADATA$START_SCAN_TIME      \n      )\n      MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE -- mandatory now!\n      \n      ;            \n\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a2fe5558-cf1a-4a2a-8c16-62ad9dc30a23",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_4",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_4\nselect * from SF_CERT_PREP.public.orders;\n\n-- Note that the fields \"order_id\" and \"subcategory\" now have null values.\n-- this is caused because the names of the columns in the source files were a bit different than the ones in the table.",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9bcad1a2-d64e-4d8a-9ed9-c8ba1af298dd",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_9",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_9\n-- In this case, the one approach is to rename the columns in the files or in the table and then reload.\n-- Since using Include_metadata does not allow any transformation\n\nCREATE OR REPLACE TABLE SF_CERT_PREP.public.ORDERS (\n    \"ORDER ID\" VARCHAR(30),\n    AMOUNT INT,\n    PROFIT INT,\n    QUANTITY INT,\n    CATEGORY VARCHAR(30),\n    \"Sub-Category\" VARCHAR(30),\n    META_FILENAME TEXT,\n    META_FILE_ROW_NUMBER INT,\n    META_FILE_CONTENT_KEY TEXT,\n    META_FILE_LAST_MODIFIED TIMESTAMP_LTZ,\n    META_START_SCAN_TIME TIMESTAMP_LTZ    \n);\n\n COPY INTO SF_CERT_PREP.public.ORDERS\n     FROM @SF_CERT_PREP.public.aws_stage\n     file_format =SF_CERT_PREP.public.ff_csv_parse_header -- mandatory to match column by name\n      pattern ='OrderDetails.csv'\n      INCLUDE_METADATA = (\nMETA_FILENAME=METADATA$FILENAME\nMETA_FILE_ROW_NUMBER=METADATA$FILE_ROW_NUMBER\nMETA_FILE_CONTENT_KEY=METADATA$FILE_CONTENT_KEY\nMETA_FILE_LAST_MODIFIED= METADATA$FILE_LAST_MODIFIED\nMETA_START_SCAN_TIME=METADATA$START_SCAN_TIME      \n      )\n      MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE -- mandatory now!\n      \n      ;        \n\nselect * from SF_CERT_PREP.public.orders;\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6b8c608c-f398-4625-aada-2cf0b374d6ed",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_24",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_24\n-- The second option is to remove the include_metadata and dynamically insert the columns with copy transform.\n-- More realistic solution.\nCREATE OR REPLACE TABLE SF_CERT_PREP.public.ORDERS (\n    ORDER_ID VARCHAR(30),\n    AMOUNT INT,\n    PROFIT INT,\n    QUANTITY INT,\n    CATEGORY VARCHAR(30),\n    Sub_Category VARCHAR(30),\n    META_FILENAME TEXT,\n    META_FILE_ROW_NUMBER INT,\n    META_FILE_CONTENT_KEY TEXT,\n    META_FILE_LAST_MODIFIED TIMESTAMP_LTZ,\n    META_START_SCAN_TIME TIMESTAMP_LTZ    \n);\n\n COPY INTO SF_CERT_PREP.public.ORDERS\n     FROM(\n        select\n         $1\n,$2\n,$3\n,$4\n,$5\n,$6\n,METADATA$FILENAME\n,METADATA$FILE_ROW_NUMBER\n,METADATA$FILE_CONTENT_KEY\n,METADATA$FILE_LAST_MODIFIED\n,METADATA$START_SCAN_TIME  from  @SF_CERT_PREP.public.aws_stage)\n     file_format =SF_CERT_PREP.public.ff_csv_skip_header\n      pattern ='OrderDetails.csv'\n      \n      \n      ;        \n\nselect * from SF_CERT_PREP.public.orders;\n",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e67e7f2e-786b-4503-8de0-ed366769b7bd",
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "codeCollapsed": false
      },
      "source": "## Copy into Stage\n\nUnload data\n\n    \n    COPY INTO { internalStage | externalStage | externalLocation }\n         FROM { [<namespace>.]<table_name> | ( <query> ) }\n    [ PARTITION BY <expr> ]\n    [ FILE_FORMAT = ( { FORMAT_NAME = '[<namespace>.]<file_format_name>' |\n                        TYPE = { CSV | JSON | PARQUET } [ formatTypeOptions ] } ) ]\n    [ copyOptions ]\n    [ VALIDATION_MODE = RETURN_ROWS ]\n    [ HEADER ]\n\n\n### Optional parameters:\n\n#### PARTITION BY expr:\n\n\nSpecifies an expression used to partition the unloaded table rows into separate files\n\n\ncopyOptions ::=\nOVERWRITE = TRUE | FALSE\n\n    DEFAULT = FALSE \n\nwhether the COPY command overwrites existing files with matching names.\n\n\nSINGLE = TRUE | FALSE\n\n    DEFAULT = FALSE \n\nwhether to generate a single file or multiple files. If FALSE, a filename prefix must be included in path.\n\n***Important***:\nIf SINGLE = TRUE, then COPY ignores the FILE_EXTENSION file format option and outputs a file simply named data\n\n\nMAX_FILE_SIZE = <num>\n\n    DEFAULT = 16MB\n    MAX = 5GB\n\nmaximum size (in bytes) of each file to be generated in parallel per thread.\nSnowflake utilizes parallel execution to optimize performance. The number of threads can’t be modified.\n\n\nINCLUDE_QUERY_ID = TRUE | FALSE\n\n    DEFAULT = FALSE\n\nuniquely identify unloaded files by including a universally unique identifier (UUID) in the filenames of unloaded data files\n\n\nDETAILED_OUTPUT = TRUE | FALSE\n\n\n    DEFAULT = FALSE\n\nTRUE: output includes a row for each file unloaded to the specified stage\nFALSE: output consists of a single row that describes the entire unload operation\n    ",
      "execution_count": null
    },
    {
      "id": "c46fbc4b-5e46-446f-8ab9-cba523aee5df",
      "cell_type": "code",
      "metadata": {
        "language": "sql",
        "resultVariableName": "dataframe_29",
        "name": "create internal stage",
        "title": "create internal stage"
      },
      "source": "%%sql -r dataframe_29\ncreate or replace stage SF_CERT_PREP.public.my_internal_stage\n DIRECTORY = (ENABLE = TRUE) ;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ee897caf-5f67-41ae-821f-3ee7344cb028",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_32",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_32\nselect * from SF_CERT_PREP.public.orders;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e1582898-b3f1-42e9-a3f6-cd4746d8ff52",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_30",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_30\ncopy into @SF_CERT_PREP.public.my_internal_stage\nfrom SF_CERT_PREP.public.orders;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "859087f3-b3b8-40d2-87be-b6af1a004359",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_31",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_31\nlist @SF_CERT_PREP.public.my_internal_stage;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7ee2c9e1-058b-4d14-8f51-447890456d67",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_33",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_33\ncopy into @SF_CERT_PREP.public.my_internal_stage/version2\nfile_Format = (type =CSV)\nheader = true\nfrom SF_CERT_PREP.public.orders;",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e9765117-cdfb-458a-a2c1-82684521eaf2",
      "cell_type": "markdown",
      "metadata": {
        "codeCollapsed": true
      },
      "source": "## Querying history\n\n### INFORMATION_SCHEMA.LOAD_HISTORY\n\n\n    select * from SF_CERT_PREP.INFORMATION_SCHEMA.LOAD_HISTORy where table_name='ORDERS';\n\nIt will show only dataloads for the current version of the table. In case table is dropped, it will not be shown here.\n\n\n### snowflake.account_usage.load_history\n\n    select * from  snowflake.account_usage.load_history where table_name='ORDERS';\n\n\nit shows the history of data loaded into tables using the COPY INTO <table> command within the last 365 days (1 year).\nThe view displays one row for each file loaded.\n\nIn most cases, latency for the view may be up to 90 minutes. \n\nThe latency for a given table’s load history in the view may be up to 2 days if both of the following conditions are true:\n\n- Fewer than 32 DML statements have been added to the given table since it was last updated in LOAD_HISTORY.\n- Fewer than 100 rows have been added to the given table since it was last updated in LOAD_HISTORY.\n\nIMPORTANT: This view does not return the history of data loaded using Snowpipe\n\n\n### snowflake.account_usage.copy_history\n\nThis Account Usage view can be used to query Snowflake data loading history for the last 365 days (1 year).\n\n- it displays load activity for COPY INTO statements and Snowpipe. \n- The view avoids the 10,000 row limitation of the LOAD_HISTORY view.\n\n    select * from SNOWFLAKE.ACCOUNT_USAGE.COPY_HISTORY where table_name='ORDERS';\n\nIn most cases, latency for the view may be up to 120 minutes. \nThe latency for a given table’s load history in the view may be up to 2 days if both of the following conditions are true:\n\n- Fewer than 32 DML statements have been added to the given table since it was last updated in LOAD_HISTORY.\n- Fewer than 100 rows have been added to the given table since it was last updated in LOAD_HISTORY.\n\n    ",
      "execution_count": null
    },
    {
      "id": "e7ccc080-65ad-4ba0-8a78-0e94b2c65d75",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_16",
        "language": "sql",
        "name": "history - INFORMATION_SCHEMA",
        "title": "history - INFORMATION_SCHEMA"
      },
      "source": "%%sql -r dataframe_16\nselect * from SF_CERT_PREP.INFORMATION_SCHEMA.LOAD_HISTORy where table_name='ORDERS';\n-- Only show live loads, all loads that happeed in tables dropped, will not be shown here",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e9a0437c-8444-4918-9ccf-8d422c30266d",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_26",
        "language": "sql",
        "name": "history - snowflake.account_usage",
        "title": "history - snowflake.account_usage"
      },
      "source": "select * from  snowflake.account_usage.load_history where table_name='ORDERS';\n-- do not include snowpipe",
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "62694703-7790-42e0-885a-675c83a62ff3",
      "cell_type": "code",
      "metadata": {
        "resultVariableName": "dataframe_27",
        "language": "sql"
      },
      "source": "%%sql -r dataframe_27\nselect * from SNOWFLAKE.ACCOUNT_USAGE.COPY_HISTORY where table_name='ORDERS';\n-- include snowpipe copy",
      "outputs": [],
      "execution_count": null
    }
  ]
}